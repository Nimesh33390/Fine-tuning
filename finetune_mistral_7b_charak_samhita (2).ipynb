{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csyv0MCjB3DP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install transformers==4.51.3\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "4a0d3a2b0a5141a9b499305b16edd5d0",
      "dcdc295d352641a1a80a073ab838e373",
      "962e025c0d0d4025bcb579a33a33acb8",
      "90ac936628674e18975594c0561641d0",
      "8d15cc98a1b745b187066266dbedc65e",
      "b231acca00224ece857355befcc6bdde",
      "2f57d7fbc3b04a0e8c06db50313c918e",
      "0ae5d55ea4894c28b0363332f4ec5b37",
      "17b2f79bf13c41d0869a5aff9cfa0560",
      "a926cc99d8c64824ad59f1b032fce580",
      "4db68b42c04c42bfbde6aea4b6096c24",
      "892fd81160ad41568fe2944a3702ccb9",
      "e3d4bcee754342d29ac63068390e3950",
      "47f5b929f8174192ae7cffb8f94457a1",
      "3a48af1550994d5ab007e209e2b1459c",
      "7bc4d0cb82a84af0a6cf0b17e4632e45",
      "0b6efbd892014620a37ff49762770a82",
      "598c95dd47fe4d16a26bb6c5c75db18a",
      "c0d6a8ba3aab4a86987723ed70d99dc5",
      "de542aa4bfd84de0bd3c32773bbbd5d6",
      "1adcde11a5fe4b3cb8bfa3825f8c875a",
      "111b6b2aa61148b48af96a2640979cd7",
      "acd378aaf8c745aaac0a3deae556cdee",
      "c65b5db4e6ee482ca20b60c0d6e77ba5",
      "ad31fe3ce7654ce28467cd6ad23cc2cc",
      "35c12d26811b4495a47727758173ae03",
      "7404875070584167b17fe8e77a2c6067",
      "fb5a95912c314075873b9ac99b1fbf3b",
      "e21818a268b44579838c4756f4c57798",
      "b5a28e7918a749c8b76682a2b4890ade",
      "1cacd48e066d44d08d6489d115c68f43",
      "c283d944a8314e0a8e88e27009908670",
      "16fe96650c954c3882bae1fdde60d8bc",
      "4e412b40883f495a8c7db886ae07de08",
      "ac02d79b6afd481bab584c31edd4b745",
      "7c2f8f1b9bd34987b64334e4d5f66476",
      "1980e6f8730647e5b37ccc6fb9b0eec3",
      "fb90bc67989845c28aab5b02fb6c44a1",
      "58a40234672f4d77b57b72024d9f5ffa",
      "ad941fce88844900907f9d37b56784b3",
      "d1bae4faf9774efea6da527665a045e9",
      "779c9b706f444914b560ad29eb207e4c",
      "81b07ee5fb344fb2ab9b28ec47741078",
      "de62ef95dd184311a519e59aebd54fe1",
      "32c81887a6c94d6a8c13e4a002781b7e",
      "6400b53bbfa44a0f89155805b793407b",
      "387583ab65114df1ac20d91481a61212",
      "5a35ed4fa4344b25bd5725d25f8f702f",
      "66da2b6a6b7a430c837f7bbf3e75f426",
      "4e9539f7ce8c4a289e9d67d1ba39ef6e",
      "c470b3866e4549f996e6e6dfa7564ed2",
      "505fa7133ca04736b8f35ee9090b5eee",
      "1ab36207d9734387ba3a5b5cbeebfde0",
      "7016b0fa10fa48c0802f21aa5e597b50",
      "cda576969e1445e18f7ced313d585889",
      "3824c5effce84178ba925cdb33173ebd",
      "df0ab54d33a94011b30fe8809f951746",
      "fc2c778b9ab24fd785077af64c9d0647",
      "814d4a9bb72743a9a63e44872117c975",
      "5cbbd982cf884becbeec5a331a005b2d",
      "3fadd76ad55e43f38bfa3c0499c7eed5",
      "1550adcb03d44d2ca199ff3b6492017c",
      "767d6c2e4aef4509acba9eb381975fbc",
      "f1fb925f5cd0416b8873b1e9672707d6",
      "07343d0524a54443ab7a7d2798053ee6",
      "75c401ff86e84f558a136ca1e4823ef4"
     ]
    },
    "id": "o1c-gSxACeix",
    "outputId": "ae808107-e37d-4a50-b4c8-ab951aa1921d"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
    "#     \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "#     \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "#     \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "#     \"unsloth/codellama-34b-bnb-4bit\",\n",
    "#     \"unsloth/tinyllama-bnb-4bit\",\n",
    "#     \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "#     \"unsloth/gemma-2b-bnb-4bit\",\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAFRUS6KCuMS",
    "outputId": "0ddf301c-33a3-44cf-b5f4-02f2bc8aa326"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/gita/Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk1C3pNSD9q4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the directory containing your JSON files\n",
    "base_dir = '/content/Datasets/Ayurveda/charak-samhita'\n",
    "\n",
    "# Initialize a list to hold all conversations\n",
    "all_conversations = []\n",
    "\n",
    "# Traverse through all subdirectories and files\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for entry in data:\n",
    "                    verse_id = entry.get('verse_id', 'Unknown')\n",
    "                    text = entry.get('text', '')\n",
    "                    # Create a prompt and response\n",
    "                    prompt = f\"Explain verse {verse_id}: {text}\"\n",
    "                    # Placeholder response; replace with actual explanation if available\n",
    "                    response = \"This verse discusses...\"\n",
    "                    conversation = {\n",
    "                        \"conversations\": [\n",
    "                            {\"from\": \"human\", \"value\": prompt},\n",
    "                            {\"from\": \"gpt\", \"value\": response}\n",
    "                        ]\n",
    "                    }\n",
    "                    all_conversations.append(conversation)\n",
    "\n",
    "# Write all conversations to a JSONL file\n",
    "with open('charak_samhita_conversations.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for convo in all_conversations:\n",
    "        json.dump(convo, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7oQLuDjHsF6",
    "outputId": "391e916a-2f16-4644-ddf7-bcc0c5c7c17a"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "64b3ccaf4f314bca8c8456d481e2129c",
      "3dab9545aae34355a60bd7377a5489e5",
      "87e1a30e79e046a0880ae3440f8e4c7a",
      "c9a69fb10521489e99fc2ed5c29beaa7",
      "abf15243d96748bf89e589a798c51ffd",
      "7d959bbdeac742aba5544d8684e3a601",
      "aac14f0abab34ca097fdb32e7cebc6fe",
      "a61259f81e234ee4abe0c30126cf9d24",
      "840d51270f704ae5b0325cb28b91b8b5",
      "c496e7ea247e4c4d9c85a15e5464af61",
      "f70179e66d6542878a3af8539ae794e3"
     ]
    },
    "id": "MQuZ0P5VI12h",
    "outputId": "5470b39a-e3d8-47a5-a65f-324d445060e0"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Load the dataset from the JSONL file\n",
    "dataset = load_dataset('json', data_files='charak_samhita_conversations.jsonl', split='train')\n",
    "\n",
    "# Define the formatting function\n",
    "def formatting_func(examples):\n",
    "    texts = []\n",
    "    # examples is a dictionary with keys corresponding to dataset columns\n",
    "    # examples[\"conversations\"] is a list of lists, where each inner list is a conversation for one example in the batch\n",
    "    if \"conversations\" in examples and isinstance(examples[\"conversations\"], list):\n",
    "        for conversation_list in examples[\"conversations\"]:\n",
    "            formatted_conversation = \"\"\n",
    "            # Ensure conversation_list is a list before iterating\n",
    "            if isinstance(conversation_list, list):\n",
    "                # conversation_list is a list of dictionaries, where each dictionary is a turn in the conversation\n",
    "                for turn in conversation_list:\n",
    "                    # Ensure turn is a dictionary before accessing keys\n",
    "                    if isinstance(turn, dict):\n",
    "                        if turn.get(\"from\") == \"human\":\n",
    "                            formatted_conversation += f\"### Human:\\n{turn.get('value', '')}\\n\"\n",
    "                        elif turn.get(\"from\") == \"gpt\":\n",
    "                            formatted_conversation += f\"### Assistant:\\n{turn.get('value', '')}\\n\"\n",
    "            texts.append(formatted_conversation + \"### End\") # Add a separator at the end\n",
    "    # Return the list of processed strings directly\n",
    "    return texts\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # Update this to the correct field name\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1, # Changed from 2 to 1\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    formatting_func = formatting_func, # Add the formatting function here\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng3WKz0JJAbr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "bb78c448",
    "outputId": "49d02d35-3c36-4c4c-af0c-80f3aa601838"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikj-883CLwO9",
    "outputId": "bd824569-05b5-449f-da45-ce321b411982"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_mistral\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_13rxMuwMnL8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/fine_tuned_mistral\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/content/fine_tuned_mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KntmYAobMvHx"
   },
   "outputs": [],
   "source": [
    "input_text = \"Explain verse 1: We shall now expound the chapter entitled...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDvgELmVM-8X",
    "outputId": "fa8d8574-a6b9-42db-b29e-2b9fe7eda668"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Import FastLanguageModel\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Load your saved model & tokenizer using FastLanguageModel\n",
    "model_dir = \"./fine_tuned_mistral\"  # or your path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model using FastLanguageModel.from_pretrained\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_dir, # Load from the saved directory\n",
    "    max_seq_length = max_seq_length, # Use the same max_seq_length as before\n",
    "    dtype = None, # Auto detect\n",
    "    load_in_4bit = True, # Use 4bit quantization\n",
    ")\n",
    "\n",
    "# 3. Move model to GPU/CPU (FastLanguageModel already handles device placement, but keeping for clarity if needed)\n",
    "# model.to(device) # FastLanguageModel handles this\n",
    "\n",
    "# 4. Prepare your prompt\n",
    "input_text = \"Explain verse 1: We shall now expound the chapter entitled...\"\n",
    "\n",
    "# 5. Tokenize and move tensors to the same device\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()} # Use model.device\n",
    "\n",
    "# 6. Generate\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,      # greedy; you can also experiment with sampling\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 7. Decode\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcBP51OqNC14"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
